input_ids     {'tensor_meta': TensorMetadata(shape=torch.Size([1, 10]), dtype=torch.int64, requires_grad=False, stride=(10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size     {'type': <class 'torch.Size'>}
getitem     {'type': <class 'int'>}
view     {'tensor_meta': TensorMetadata(shape=torch.Size([1, 10]), dtype=torch.int64, requires_grad=False, stride=(10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_1     {'type': <class 'torch.Size'>}
getitem_1     {'type': <class 'int'>}
getitem_2     {'type': <class 'int'>}
add     {'type': <class 'int'>}
getattr_1     {'type': <class 'torch.device'>}
arange     {'tensor_meta': TensorMetadata(shape=torch.Size([10]), dtype=torch.int64, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
unsqueeze     {'tensor_meta': TensorMetadata(shape=torch.Size([1, 10]), dtype=torch.int64, requires_grad=False, stride=(10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_3     {'type': <class 'int'>}
view_1     {'tensor_meta': TensorMetadata(shape=torch.Size([1, 10]), dtype=torch.int64, requires_grad=False, stride=(10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
wte     {'nn_module_stack': OrderedDict([('wte', <class 'torch.nn.modules.sparse.Embedding'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
wpe     {'nn_module_stack': OrderedDict([('wpe', <class 'torch.nn.modules.sparse.Embedding'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_1     {'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
drop     {'nn_module_stack': OrderedDict([('drop', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_2     {'type': <class 'int'>}
add_2     {'type': <class 'torch.Size'>}
h_0_ln_1     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_3     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_4     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_3     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_0_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_4     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_2     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_0_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_3     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_5     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_6     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_7     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_5     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_8     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_4     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_4     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_6     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_9     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_5     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_5     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_1     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_7     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_10     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_6     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_6     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_2     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_8     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_1     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_2     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_3     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_9     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_10     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_0_attn_bias     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_11     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_4     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_5     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_6     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_1     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_7     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_8     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_1     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_9     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_1     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_0_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_1     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_3     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_11     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_12     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_7     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_7     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_12     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_13     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_8     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_0_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_13     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_8     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_0_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_1     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_9     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_0_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.0.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_9     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_0_ln_2     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_14     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_14     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_10     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_0_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_15     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_10     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_0_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_2     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_11     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_2     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_1     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_11     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_2     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_12     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_3     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_16     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_15     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_13     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_0_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_17     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_12     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_0_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_3     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_13     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_0_mlp_dropout     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.0.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.0.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_14     {'nn_module_stack': OrderedDict([('h.0', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_1_ln_1     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_18     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_16     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_15     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_1_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_19     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_14     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_1_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_4     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_15     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split_1     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_17     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_18     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_19     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_20     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_20     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_16     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_16     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_4     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_21     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_21     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_17     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_17     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_5     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_22     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_22     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_18     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_18     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_6     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose_1     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_2     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_23     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_3     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_10     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_11     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full_2     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv_1     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_24     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_25     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_1_attn_bias     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub_1     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_23     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_12     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo_1     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_13     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_14     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_3     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_15     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to_2     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_16     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_3     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where_1     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax_1     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_17     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_2     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_1_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_3     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_7     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous_1     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_26     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_24     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_19     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_19     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_27     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_25     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_20     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_1_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_28     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_20     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_1_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_5     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_21     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_1_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.1.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_21     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_1_ln_2     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_29     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_26     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_22     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_1_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_30     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_22     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_1_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_6     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_23     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_4     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_4     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_5     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_23     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_6     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh_1     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_24     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_7     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_31     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_27     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_25     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_1_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_32     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_24     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_1_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_7     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_25     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_1_mlp_dropout     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.1.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.1.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_26     {'nn_module_stack': OrderedDict([('h.1', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_2_ln_1     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_33     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_28     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_27     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_2_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_34     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_26     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_2_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_8     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_27     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split_2     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_29     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_30     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_31     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_35     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_32     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_28     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_28     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_8     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_36     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_33     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_29     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_29     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_9     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_37     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_34     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_30     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_30     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_10     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose_2     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_4     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_38     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_5     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_18     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_19     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full_4     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv_2     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_39     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_40     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_2_attn_bias     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub_2     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_35     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_20     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo_2     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_21     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_22     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_5     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_23     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to_4     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_24     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_5     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where_2     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax_2     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_25     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_3     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_2_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_5     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_11     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous_2     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_41     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_36     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_31     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_31     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_42     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_37     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_32     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_2_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_43     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_32     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_2_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_9     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_33     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_2_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.2.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_33     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_2_ln_2     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_44     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_38     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_34     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_2_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_45     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_34     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_2_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_10     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_35     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_8     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_6     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_9     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_35     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_10     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh_2     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_36     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_11     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_46     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_39     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_37     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_2_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_47     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_36     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_2_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_11     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_37     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_2_mlp_dropout     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.2.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.2.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_38     {'nn_module_stack': OrderedDict([('h.2', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_3_ln_1     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_48     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_40     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_39     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_3_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_49     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_38     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_3_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_12     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_39     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split_3     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_41     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_42     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_43     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_50     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_44     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_40     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_40     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_12     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_51     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_45     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_41     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_41     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_13     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_52     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_46     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_42     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_42     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_14     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose_3     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_6     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_53     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_7     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_26     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_27     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full_6     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv_3     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_54     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_55     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_3_attn_bias     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub_3     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_47     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_28     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo_3     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_29     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_30     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_7     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_31     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to_6     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_32     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_7     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where_3     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax_3     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_33     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_4     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_3_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_7     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_15     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous_3     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_56     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_48     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_43     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_43     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_57     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_49     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_44     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_3_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_58     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_44     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_3_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_13     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_45     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_3_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.3.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_45     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_3_ln_2     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_59     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_50     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_46     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_3_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_60     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_46     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_3_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_14     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_47     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_12     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_8     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_13     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_47     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_14     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh_3     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_48     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_15     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_61     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_51     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_49     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_3_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_62     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_48     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_3_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_15     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_49     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_3_mlp_dropout     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.3.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.3.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_50     {'nn_module_stack': OrderedDict([('h.3', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_4_ln_1     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_63     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_52     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_51     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_4_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_64     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_50     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_4_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_16     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_51     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split_4     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_53     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_54     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_55     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_65     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_56     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_52     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_52     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_16     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_66     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_57     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_53     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_53     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_17     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_67     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_58     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_54     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_54     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_18     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose_4     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_8     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_68     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_9     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_34     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_35     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full_8     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv_4     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_69     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_70     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_4_attn_bias     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub_4     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_59     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_36     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo_4     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_37     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_38     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_9     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_39     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to_8     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_40     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_9     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where_4     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax_4     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_41     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_5     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_4_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_9     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_19     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous_4     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_71     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_60     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_55     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_55     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_72     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_61     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_56     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_4_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_73     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_56     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_4_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_17     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_57     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_4_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.4.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_57     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_4_ln_2     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_74     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_62     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_58     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_4_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_75     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_58     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_4_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_18     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_59     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_16     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_10     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_17     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_59     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_18     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh_4     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_60     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_19     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_76     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_63     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_61     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_4_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_77     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_60     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_4_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_19     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_61     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_4_mlp_dropout     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.4.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.4.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_62     {'nn_module_stack': OrderedDict([('h.4', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_5_ln_1     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_78     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_64     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_63     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_5_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_79     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_62     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_5_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_20     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_63     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split_5     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_65     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_66     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_67     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_80     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_68     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_64     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_64     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_20     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_81     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_69     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_65     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_65     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_21     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_82     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_70     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_66     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_66     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_22     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose_5     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_10     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_83     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_11     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_42     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_43     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full_10     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv_5     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_84     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_85     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_5_attn_bias     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub_5     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_71     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_44     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo_5     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_45     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_46     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_11     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_47     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to_10     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_48     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_11     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where_5     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax_5     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_49     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_6     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_5_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_11     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_23     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous_5     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_86     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_72     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_67     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_67     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_87     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_73     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_68     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_5_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_88     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_68     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_5_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_21     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_69     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_5_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.5.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_69     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_5_ln_2     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_89     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_74     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_70     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_5_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_90     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_70     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_5_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_22     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_71     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_20     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_12     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_21     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_71     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_22     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh_5     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_72     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_23     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_91     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_75     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_73     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_5_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_92     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_72     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_5_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_23     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_73     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_5_mlp_dropout     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.5.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.5.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_74     {'nn_module_stack': OrderedDict([('h.5', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_6_ln_1     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_93     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_76     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_75     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_6_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_94     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_74     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_6_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_24     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_75     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split_6     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_77     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_78     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_79     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_95     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_80     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_76     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_76     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_24     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_96     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_81     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_77     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_77     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_25     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_97     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_82     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_78     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_78     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_26     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose_6     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_12     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_98     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_13     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_50     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_51     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full_12     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv_6     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_99     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_100     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_6_attn_bias     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub_6     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_83     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_52     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo_6     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_53     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_54     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_13     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_55     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to_12     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_56     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_13     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where_6     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax_6     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_57     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_7     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_6_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_13     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_27     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous_6     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_101     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_84     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_79     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_79     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_102     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_85     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_80     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_6_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_103     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_80     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_6_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_25     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_81     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_6_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.6.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_81     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_6_ln_2     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_104     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_86     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_82     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_6_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_105     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_82     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_6_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_26     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_83     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_24     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_14     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_25     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_83     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_26     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh_6     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_84     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_27     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_106     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_87     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_85     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_6_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_107     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_84     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_6_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_27     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_85     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_6_mlp_dropout     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.6.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.6.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_86     {'nn_module_stack': OrderedDict([('h.6', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_7_ln_1     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_108     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_88     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_87     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_7_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_109     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_86     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_7_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_28     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_87     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split_7     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_89     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_90     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_91     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_110     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_92     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_88     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_88     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_28     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_111     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_93     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_89     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_89     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_29     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_112     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_94     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_90     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_90     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_30     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose_7     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_14     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_113     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_15     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_58     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_59     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full_14     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv_7     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_114     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_115     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_7_attn_bias     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub_7     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_95     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_60     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo_7     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_61     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_62     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_15     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_63     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to_14     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_64     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_15     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where_7     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax_7     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_65     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_8     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_7_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_15     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_31     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous_7     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_116     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_96     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_91     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_91     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_117     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_97     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_92     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_7_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_118     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_92     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_7_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_29     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_93     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_7_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.7.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_93     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_7_ln_2     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_119     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_98     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_94     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_7_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_120     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_94     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_7_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_30     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_95     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_28     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_16     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_29     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_95     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_30     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh_7     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_96     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_31     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_121     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_99     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_97     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_7_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_122     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_96     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_7_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_31     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_97     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_7_mlp_dropout     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.7.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.7.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_98     {'nn_module_stack': OrderedDict([('h.7', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_8_ln_1     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_123     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_100     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_99     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_8_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_124     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_98     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_8_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_32     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_99     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split_8     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_101     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_102     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_103     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_125     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_104     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_100     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_100     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_32     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_126     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_105     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_101     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_101     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_33     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_127     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_106     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_102     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_102     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_34     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose_8     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_16     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_128     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_17     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_66     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_67     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full_16     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv_8     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_129     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_130     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_8_attn_bias     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub_8     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_107     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_68     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo_8     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_69     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_70     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_17     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_71     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to_16     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_72     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_17     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where_8     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax_8     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_73     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_9     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_8_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_17     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_35     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous_8     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_131     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_108     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_103     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_103     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_132     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_109     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_104     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_8_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_133     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_104     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_8_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_33     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_105     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_8_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.8.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_105     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_8_ln_2     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_134     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_110     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_106     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_8_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_135     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_106     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_8_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_34     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_107     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_32     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_18     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_33     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_107     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_34     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh_8     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_108     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_35     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_136     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_111     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_109     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_8_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_137     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_108     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_8_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_35     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_109     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_8_mlp_dropout     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.8.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.8.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_110     {'nn_module_stack': OrderedDict([('h.8', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_9_ln_1     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_138     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_112     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_111     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_9_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_139     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_110     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_9_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_36     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_111     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split_9     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_113     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_114     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_115     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_140     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_116     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_112     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_112     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_36     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_141     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_117     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_113     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_113     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_37     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_142     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_118     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_114     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_114     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_38     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose_9     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_18     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_143     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_19     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_74     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_75     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full_18     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv_9     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_144     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_145     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_9_attn_bias     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub_9     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_119     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_76     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo_9     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_77     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_78     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_19     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_79     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to_18     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_80     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_19     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where_9     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax_9     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_81     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_10     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_9_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_19     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_39     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous_9     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_146     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_120     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_115     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_115     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_147     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_121     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_116     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_9_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_148     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_116     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_9_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_37     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_117     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_9_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.9.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_117     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_9_ln_2     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_149     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_122     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_118     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_9_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_150     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_118     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_9_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_38     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_119     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_36     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_20     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_37     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_119     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_38     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh_9     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_120     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_39     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_151     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_123     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_121     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_9_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_152     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_120     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_9_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_39     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_121     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_9_mlp_dropout     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.9.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.9.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_122     {'nn_module_stack': OrderedDict([('h.9', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_10_ln_1     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_153     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_124     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_123     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_10_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_154     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_122     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_10_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_40     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_123     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split_10     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_125     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_126     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_127     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_155     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_128     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_124     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_124     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_40     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_156     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_129     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_125     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_125     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_41     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_157     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_130     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_126     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_126     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_42     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose_10     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_20     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_158     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_21     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_82     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_83     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full_20     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv_10     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_159     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_160     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_10_attn_bias     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub_10     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_131     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_84     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo_10     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_85     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_86     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_21     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_87     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to_20     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_88     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_21     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where_10     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax_10     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_89     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_11     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_10_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_21     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_43     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous_10     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_161     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_132     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_127     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_127     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_162     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_133     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_128     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_10_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_163     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_128     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_10_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_41     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_129     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_10_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.10.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_129     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_10_ln_2     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_164     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_134     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_130     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_10_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_165     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_130     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_10_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_42     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_131     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_40     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_22     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_41     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_131     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_42     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh_10     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_132     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_43     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_166     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_135     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_133     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_10_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_167     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_132     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_10_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_43     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_133     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_10_mlp_dropout     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.10.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.10.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_134     {'nn_module_stack': OrderedDict([('h.10', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_11_ln_1     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.ln_1', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_168     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_136     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_135     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_11_attn_c_attn_bias     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([2304]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_169     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_134     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_11_attn_c_attn_weight     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_44     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 2304]), dtype=torch.float32, requires_grad=True, stride=(2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_135     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_attn', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 2304]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
split_11     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': (TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={})), 'type': <class 'tuple'>}
getitem_137     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_138     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getitem_139     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_170     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_140     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_136     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_136     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_44     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_171     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_141     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_137     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_137     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_45     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_172     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_142     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_138     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_138     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 2304, 64, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_46     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
transpose_11     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 64, 10]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 1, 2304), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_22     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_173     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
pow_23     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_90     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
getattr_91     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
full_22     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
truediv_11     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_174     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
size_175     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
h_11_attn_bias     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 1024, 1024]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=torch.channels_last, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
sub_11     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'int'>}
getitem_143     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 1, 10, 10]), dtype=torch.bool, requires_grad=False, stride=(1048576, 1048576, 1024, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_92     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
finfo_11     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.finfo'>}
getattr_93     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'float'>}
getattr_94     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
full_23     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_95     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.device'>}
to_22     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([]), dtype=torch.float32, requires_grad=False, stride=(), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_96     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
to_23     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
where_11     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
softmax_11     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
getattr_97     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.dtype'>}
type_12     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_11_attn_attn_dropout     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.attn_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 10]), dtype=torch.float32, requires_grad=True, stride=(1200, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
matmul_23     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 640, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
permute_47     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 64, 640, 1), memory_format=None, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
contiguous_11     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 12, 64]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 64, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_176     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
getitem_144     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
add_139     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'type': <class 'torch.Size'>}
view_139     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_177     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_145     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_140     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_11_attn_c_proj_bias     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_178     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_140     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_11_attn_c_proj_weight     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_45     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_141     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_11_attn_resid_dropout     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.attn', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>), ('h.11.attn.resid_dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_141     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_11_ln_2     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.ln_2', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_179     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_146     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_142     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_11_mlp_c_fc_bias     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_180     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_142     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_11_mlp_c_fc_weight     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_46     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_143     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_fc', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_44     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
pow_24     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_45     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_143     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_46     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
tanh_11     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_144     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
mul_47     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.act', <class 'transformers.activations.NewGELUActivation'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 3072]), dtype=torch.float32, requires_grad=True, stride=(30720, 3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
size_181     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
getitem_147     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
add_145     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'torch.Size'>}
h_11_mlp_c_proj_bias     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([768]), dtype=torch.float32, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
size_182     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'type': <class 'int'>}
view_144     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 3072]), dtype=torch.float32, requires_grad=True, stride=(3072, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_11_mlp_c_proj_weight     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([3072, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.nn.parameter.Parameter'>}
addmm_47     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([10, 768]), dtype=torch.float32, requires_grad=True, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_145     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.c_proj', <class 'transformers.pytorch_utils.Conv1D'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
h_11_mlp_dropout     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>), ('h.11.mlp', <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>), ('h.11.mlp.dropout', <class 'torch.nn.modules.dropout.Dropout'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
add_146     {'nn_module_stack': OrderedDict([('h.11', <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
ln_f     {'nn_module_stack': OrderedDict([('ln_f', <class 'torch.nn.modules.normalization.LayerNorm'>)]), 'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
view_146     {'tensor_meta': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'type': <class 'torch.Tensor'>}
output     {'tensor_meta': {'last_hidden_state': TensorMetadata(shape=torch.Size([1, 10, 768]), dtype=torch.float32, requires_grad=True, stride=(7680, 768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'past_key_values': ((TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})), (TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})), (TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})), (TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})), (TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})), (TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})), (TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})), (TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})), (TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})), (TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})), (TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})), (TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={}), TensorMetadata(shape=torch.Size([1, 12, 10, 64]), dtype=torch.float32, requires_grad=True, stride=(23040, 64, 2304, 1), memory_format=None, is_quantized=False, qparams={})))}, 'type': <class 'torch.fx.immutable_collections.immutable_dict'>}