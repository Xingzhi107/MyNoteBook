ssh root@172.20.20.220

ssh root@172.20.21.165（拉不了镜像）cha
ssh root@172.20.21.157（拉不了镜像）×

ssh root@172.20.21.21(下了运行不起来)
ssh root@172.20.21.29（下了运行不起来）

conda环境都废了

这个错误提示表明您的/var/lib/dpkg/info/xserver-common.list文件被损坏了。要解决这个问题，您需要删除这个文件，然后重新安装xserver-common包。1 您可以按照以下的步骤进行操作：

删除损坏的文件，运行以下的命令：
sudo rm /var/lib/dpkg/info/xserver-common.list

重新安装xserver-common包，运行以下的命令：
sudo apt --reinstall install xserver-common

export PATH=/usr/local/cuda-11.1/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64:$LD_LIBRARY_PATH
export CUDA_HOME=/usr/local/cuda-11.1

sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11.1/lib64/
sudo cp cuda/include/cudnn* /usr/local/cuda-11.1/include/

sudo chmod a+r /usr/local/cuda-11.1/include/cudnn*
sudo chmod a+r /usr/local/cuda-11.1/lib64/libcudnn*

bash build.sh -e gpu -S on

mpirun -n 4 --allow-run-as-root python test.py

CUDA_VISIBLE_DEVICES=6,7 mpirun -n 2 --allow-run-as-root \
                  python test.py \
                         --use_ascend False \
                         --jit True \
                         --do_train True \
                         --lr 2e-5 \
                         --warmup_steps 10000 \
                         --train_batch_size 256 \
                         --epochs 15 \
                         --save_steps 10000 \
                         --do_load_ckpt False \
                         --config  bert-base/config.json


Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.attention.dense1.weight   Torch:bert.encoder.layer.0.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.attention.dense1.bias     Torch:bert.encoder.layer.0.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.attention.dense2.weight   Torch:bert.encoder.layer.0.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.attention.dense2.bias     Torch:bert.encoder.layer.0.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.attention.dense3.weight   Torch:bert.encoder.layer.0.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.attention.dense3.bias     Torch:bert.encoder.layer.0.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.attention.projection.weight   Torch:bert.encoder.layer.0.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.attention.projection.bias     Torch:bert.encoder.layer.0.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.layernorm2.gamma      Torch:bert.encoder.layer.0.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.layernorm2.beta   Torch:bert.encoder.layer.0.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.output.mapping.weight     Torch:bert.encoder.layer.0.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.output.mapping.bias   Torch:bert.encoder.layer.0.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.output.projection.weight      Torch:bert.encoder.layer.0.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.output.projection.bias    Torch:bert.encoder.layer.0.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.layernorm1.gamma      Torch:bert.encoder.layer.0.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.0.layernorm1.beta   Torch:bert.encoder.layer.0.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.attention.dense1.weight   Torch:bert.encoder.layer.1.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.attention.dense1.bias     Torch:bert.encoder.layer.1.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.attention.dense2.weight   Torch:bert.encoder.layer.1.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.attention.dense2.bias     Torch:bert.encoder.layer.1.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.attention.dense3.weight   Torch:bert.encoder.layer.1.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.attention.dense3.bias     Torch:bert.encoder.layer.1.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.attention.projection.weight   Torch:bert.encoder.layer.1.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.attention.projection.bias     Torch:bert.encoder.layer.1.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.layernorm2.gamma      Torch:bert.encoder.layer.1.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.layernorm2.beta   Torch:bert.encoder.layer.1.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.output.mapping.weight     Torch:bert.encoder.layer.1.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.output.mapping.bias   Torch:bert.encoder.layer.1.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.output.projection.weight      Torch:bert.encoder.layer.1.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.output.projection.bias    Torch:bert.encoder.layer.1.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.layernorm1.gamma      Torch:bert.encoder.layer.1.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.1.layernorm1.beta   Torch:bert.encoder.layer.1.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.attention.dense1.weight   Torch:bert.encoder.layer.2.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.attention.dense1.bias     Torch:bert.encoder.layer.2.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.attention.dense2.weight   Torch:bert.encoder.layer.2.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.attention.dense2.bias     Torch:bert.encoder.layer.2.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.attention.dense3.weight   Torch:bert.encoder.layer.2.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.attention.dense3.bias     Torch:bert.encoder.layer.2.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.attention.projection.weight   Torch:bert.encoder.layer.2.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.attention.projection.bias     Torch:bert.encoder.layer.2.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.layernorm2.gamma      Torch:bert.encoder.layer.2.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.layernorm2.beta   Torch:bert.encoder.layer.2.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.output.mapping.weight     Torch:bert.encoder.layer.2.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.output.mapping.bias   Torch:bert.encoder.layer.2.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.output.projection.weight      Torch:bert.encoder.layer.2.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.output.projection.bias    Torch:bert.encoder.layer.2.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.layernorm1.gamma      Torch:bert.encoder.layer.2.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.2.layernorm1.beta   Torch:bert.encoder.layer.2.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.attention.dense1.weight   Torch:bert.encoder.layer.3.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.attention.dense1.bias     Torch:bert.encoder.layer.3.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.attention.dense2.weight   Torch:bert.encoder.layer.3.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.attention.dense2.bias     Torch:bert.encoder.layer.3.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.attention.dense3.weight   Torch:bert.encoder.layer.3.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.attention.dense3.bias     Torch:bert.encoder.layer.3.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.attention.projection.weight   Torch:bert.encoder.layer.3.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.attention.projection.bias     Torch:bert.encoder.layer.3.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.layernorm2.gamma      Torch:bert.encoder.layer.3.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.layernorm2.beta   Torch:bert.encoder.layer.3.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.output.mapping.weight     Torch:bert.encoder.layer.3.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.output.mapping.bias   Torch:bert.encoder.layer.3.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.output.projection.weight      Torch:bert.encoder.layer.3.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.output.projection.bias    Torch:bert.encoder.layer.3.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.layernorm1.gamma      Torch:bert.encoder.layer.3.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.3.layernorm1.beta   Torch:bert.encoder.layer.3.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.attention.dense1.weight   Torch:bert.encoder.layer.4.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.attention.dense1.bias     Torch:bert.encoder.layer.4.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.attention.dense2.weight   Torch:bert.encoder.layer.4.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.attention.dense2.bias     Torch:bert.encoder.layer.4.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.attention.dense3.weight   Torch:bert.encoder.layer.4.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.attention.dense3.bias     Torch:bert.encoder.layer.4.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.attention.projection.weight   Torch:bert.encoder.layer.4.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.attention.projection.bias     Torch:bert.encoder.layer.4.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.layernorm2.gamma      Torch:bert.encoder.layer.4.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.layernorm2.beta   Torch:bert.encoder.layer.4.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.output.mapping.weight     Torch:bert.encoder.layer.4.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.output.mapping.bias   Torch:bert.encoder.layer.4.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.output.projection.weight      Torch:bert.encoder.layer.4.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.output.projection.bias    Torch:bert.encoder.layer.4.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.layernorm1.gamma      Torch:bert.encoder.layer.4.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.4.layernorm1.beta   Torch:bert.encoder.layer.4.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.attention.dense1.weight   Torch:bert.encoder.layer.5.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.attention.dense1.bias     Torch:bert.encoder.layer.5.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.attention.dense2.weight   Torch:bert.encoder.layer.5.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.attention.dense2.bias     Torch:bert.encoder.layer.5.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.attention.dense3.weight   Torch:bert.encoder.layer.5.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.attention.dense3.bias     Torch:bert.encoder.layer.5.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.attention.projection.weight   Torch:bert.encoder.layer.5.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.attention.projection.bias     Torch:bert.encoder.layer.5.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.layernorm2.gamma      Torch:bert.encoder.layer.5.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.layernorm2.beta   Torch:bert.encoder.layer.5.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.output.mapping.weight     Torch:bert.encoder.layer.5.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.output.mapping.bias   Torch:bert.encoder.layer.5.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.output.projection.weight      Torch:bert.encoder.layer.5.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.output.projection.bias    Torch:bert.encoder.layer.5.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.layernorm1.gamma      Torch:bert.encoder.layer.5.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.5.layernorm1.beta   Torch:bert.encoder.layer.5.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.attention.dense1.weight   Torch:bert.encoder.layer.6.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.attention.dense1.bias     Torch:bert.encoder.layer.6.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.attention.dense2.weight   Torch:bert.encoder.layer.6.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.attention.dense2.bias     Torch:bert.encoder.layer.6.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.attention.dense3.weight   Torch:bert.encoder.layer.6.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.attention.dense3.bias     Torch:bert.encoder.layer.6.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.attention.projection.weight   Torch:bert.encoder.layer.6.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.attention.projection.bias     Torch:bert.encoder.layer.6.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.layernorm2.gamma      Torch:bert.encoder.layer.6.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.layernorm2.beta   Torch:bert.encoder.layer.6.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.output.mapping.weight     Torch:bert.encoder.layer.6.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.output.mapping.bias   Torch:bert.encoder.layer.6.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.output.projection.weight      Torch:bert.encoder.layer.6.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.output.projection.bias    Torch:bert.encoder.layer.6.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.layernorm1.gamma      Torch:bert.encoder.layer.6.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.6.layernorm1.beta   Torch:bert.encoder.layer.6.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.attention.dense1.weight   Torch:bert.encoder.layer.7.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.attention.dense1.bias     Torch:bert.encoder.layer.7.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.attention.dense2.weight   Torch:bert.encoder.layer.7.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.attention.dense2.bias     Torch:bert.encoder.layer.7.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.attention.dense3.weight   Torch:bert.encoder.layer.7.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.attention.dense3.bias     Torch:bert.encoder.layer.7.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.attention.projection.weight   Torch:bert.encoder.layer.7.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.attention.projection.bias     Torch:bert.encoder.layer.7.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.layernorm2.gamma      Torch:bert.encoder.layer.7.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.layernorm2.beta   Torch:bert.encoder.layer.7.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.output.mapping.weight     Torch:bert.encoder.layer.7.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.output.mapping.bias   Torch:bert.encoder.layer.7.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.output.projection.weight      Torch:bert.encoder.layer.7.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.output.projection.bias    Torch:bert.encoder.layer.7.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.layernorm1.gamma      Torch:bert.encoder.layer.7.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.7.layernorm1.beta   Torch:bert.encoder.layer.7.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.attention.dense1.weight   Torch:bert.encoder.layer.8.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.attention.dense1.bias     Torch:bert.encoder.layer.8.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.attention.dense2.weight   Torch:bert.encoder.layer.8.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.attention.dense2.bias     Torch:bert.encoder.layer.8.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.attention.dense3.weight   Torch:bert.encoder.layer.8.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.attention.dense3.bias     Torch:bert.encoder.layer.8.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.attention.projection.weight   Torch:bert.encoder.layer.8.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.attention.projection.bias     Torch:bert.encoder.layer.8.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.layernorm2.gamma      Torch:bert.encoder.layer.8.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.layernorm2.beta   Torch:bert.encoder.layer.8.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.output.mapping.weight     Torch:bert.encoder.layer.8.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.output.mapping.bias   Torch:bert.encoder.layer.8.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.output.projection.weight      Torch:bert.encoder.layer.8.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.output.projection.bias    Torch:bert.encoder.layer.8.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.layernorm1.gamma      Torch:bert.encoder.layer.8.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.8.layernorm1.beta   Torch:bert.encoder.layer.8.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.attention.dense1.weight   Torch:bert.encoder.layer.9.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.attention.dense1.bias     Torch:bert.encoder.layer.9.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.attention.dense2.weight   Torch:bert.encoder.layer.9.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.attention.dense2.bias     Torch:bert.encoder.layer.9.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.attention.dense3.weight   Torch:bert.encoder.layer.9.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.attention.dense3.bias     Torch:bert.encoder.layer.9.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.attention.projection.weight   Torch:bert.encoder.layer.9.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.attention.projection.bias     Torch:bert.encoder.layer.9.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.layernorm2.gamma      Torch:bert.encoder.layer.9.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.layernorm2.beta   Torch:bert.encoder.layer.9.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.output.mapping.weight     Torch:bert.encoder.layer.9.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.output.mapping.bias   Torch:bert.encoder.layer.9.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.output.projection.weight      Torch:bert.encoder.layer.9.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.output.projection.bias    Torch:bert.encoder.layer.9.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.layernorm1.gamma      Torch:bert.encoder.layer.9.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.9.layernorm1.beta   Torch:bert.encoder.layer.9.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.attention.dense1.weight      Torch:bert.encoder.layer.10.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.attention.dense1.bias    Torch:bert.encoder.layer.10.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.attention.dense2.weight      Torch:bert.encoder.layer.10.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.attention.dense2.bias    Torch:bert.encoder.layer.10.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.attention.dense3.weight      Torch:bert.encoder.layer.10.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.attention.dense3.bias    Torch:bert.encoder.layer.10.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.attention.projection.weight      Torch:bert.encoder.layer.10.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.attention.projection.bias    Torch:bert.encoder.layer.10.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.layernorm2.gamma     Torch:bert.encoder.layer.10.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.layernorm2.beta      Torch:bert.encoder.layer.10.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.output.mapping.weight    Torch:bert.encoder.layer.10.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.output.mapping.bias      Torch:bert.encoder.layer.10.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.output.projection.weight     Torch:bert.encoder.layer.10.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.output.projection.bias   Torch:bert.encoder.layer.10.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.layernorm1.gamma     Torch:bert.encoder.layer.10.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.10.layernorm1.beta      Torch:bert.encoder.layer.10.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.attention.dense1.weight      Torch:bert.encoder.layer.11.attention.self.query.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.attention.dense1.bias    Torch:bert.encoder.layer.11.attention.self.query.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.attention.dense2.weight      Torch:bert.encoder.layer.11.attention.self.key.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.attention.dense2.bias    Torch:bert.encoder.layer.11.attention.self.key.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.attention.dense3.weight      Torch:bert.encoder.layer.11.attention.self.value.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.attention.dense3.bias    Torch:bert.encoder.layer.11.attention.self.value.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.attention.projection.weight      Torch:bert.encoder.layer.11.attention.output.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.attention.projection.bias    Torch:bert.encoder.layer.11.attention.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.layernorm2.gamma     Torch:bert.encoder.layer.11.attention.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.layernorm2.beta      Torch:bert.encoder.layer.11.attention.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.output.mapping.weight    Torch:bert.encoder.layer.11.intermediate.dense.weight with shape (768, 3072)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.output.mapping.bias      Torch:bert.encoder.layer.11.intermediate.dense.bias with shape (3072,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.output.projection.weight     Torch:bert.encoder.layer.11.output.dense.weight with shape (3072, 768)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.output.projection.bias   Torch:bert.encoder.layer.11.output.dense.bias with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.layernorm1.gamma     Torch:bert.encoder.layer.11.output.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.bert_encoder.encoder.blocks.11.layernorm1.beta      Torch:bert.encoder.layer.11.output.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.word_embedding.embedding_table      Torch:bert.embeddings.word_embeddings.weight with shape (30522, 768)
Mapping table Mindspore:bert.embedding_postprocessor.full_position_embedding.embedding_table     Torch:bert.embeddings.position_embeddings.weight with shape (512, 768)
Mapping table Mindspore:bert.embedding_postprocessor.token_type_embedding.embedding_table    Torch:bert.embeddings.token_type_embeddings.weight with shape (2, 768)
Mapping table Mindspore:bert.embedding_postprocessor.layernorm.gamma     Torch:bert.embeddings.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.embedding_postprocessor.layernorm.beta      Torch:bert.embeddings.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.dense.weight                Torch:bert.pooler.dense.weight       with shape (768, 768)
Mapping table Mindspore:bert.dense.bias                  Torch:bert.pooler.dense.bias         with shape (768,)
Mapping table Mindspore:bert.mlmloss.dense.weight        Torch:cls.predictions.transform.dense.weight with shape (768, 768)
Mapping table Mindspore:bert.mlmloss.dense.bias          Torch:cls.predictions.transform.dense.bias with shape (768,)
Mapping table Mindspore:bert.mlmloss.layernorm.gamma     Torch:cls.predictions.transform.LayerNorm.gamma with shape (768,)
Mapping table Mindspore:bert.mlmloss.layernorm.beta      Torch:cls.predictions.transform.LayerNorm.beta with shape (768,)
Mapping table Mindspore:bert.mlmloss.vocab_dense.weight      Torch:cls.predictions.decoder.weight with shape (30522, 768)
Convert finished, the output is saved to test/bert-base/mindspore_bert.ckpt


python mindformers/mindformers/tools/dataset_preprocess/gpt2/wikitext2_data_process.py --input_file="test/data/wikitext-2/wiki.train.tokens" --output_file="test/data/wikitext-2/wikitext-2.mindrecord"


2024-01-29 05:53:03,873 - mindformers[mindformers/trainer/optimizer_grouped_parameters.py:113] - INFO - Param groups = {
  "decay": {
    "weight_decay": 0.1,
    "params": [
      "backbone.embedding.word_embedding.embedding_table",
      "backbone.embedding.position_embedding.embedding_table",
      "backbone.blocks.0.attention.projection.weight",
      "backbone.blocks.0.attention.dense1.weight",
      "backbone.blocks.0.attention.dense2.weight",
      "backbone.blocks.0.attention.dense3.weight",
      "backbone.blocks.0.output.mapping.weight",
      "backbone.blocks.0.output.projection.weight",
      "backbone.blocks.1.attention.projection.weight",
      "backbone.blocks.1.attention.dense1.weight",
      "backbone.blocks.1.attention.dense2.weight",
      "backbone.blocks.1.attention.dense3.weight",
      "backbone.blocks.1.output.mapping.weight",
      "backbone.blocks.1.output.projection.weight",
      "backbone.blocks.2.attention.projection.weight",
      "backbone.blocks.2.attention.dense1.weight",
      "backbone.blocks.2.attention.dense2.weight",
      "backbone.blocks.2.attention.dense3.weight",
      "backbone.blocks.2.output.mapping.weight",
      "backbone.blocks.2.output.projection.weight",
      "backbone.blocks.3.attention.projection.weight",
      "backbone.blocks.3.attention.dense1.weight",
      "backbone.blocks.3.attention.dense2.weight",
      "backbone.blocks.3.attention.dense3.weight",
      "backbone.blocks.3.output.mapping.weight",
      "backbone.blocks.3.output.projection.weight",
      "backbone.blocks.4.attention.projection.weight",
      "backbone.blocks.4.attention.dense1.weight",
      "backbone.blocks.4.attention.dense2.weight",
      "backbone.blocks.4.attention.dense3.weight",
      "backbone.blocks.4.output.mapping.weight",
      "backbone.blocks.4.output.projection.weight",
      "backbone.blocks.5.attention.projection.weight",
      "backbone.blocks.5.attention.dense1.weight",
      "backbone.blocks.5.attention.dense2.weight",
      "backbone.blocks.5.attention.dense3.weight",
      "backbone.blocks.5.output.mapping.weight",
      "backbone.blocks.5.output.projection.weight",
      "backbone.blocks.6.attention.projection.weight",
      "backbone.blocks.6.attention.dense1.weight",
      "backbone.blocks.6.attention.dense2.weight",
      "backbone.blocks.6.attention.dense3.weight",
      "backbone.blocks.6.output.mapping.weight",
      "backbone.blocks.6.output.projection.weight",
      "backbone.blocks.7.attention.projection.weight",
      "backbone.blocks.7.attention.dense1.weight",
      "backbone.blocks.7.attention.dense2.weight",
      "backbone.blocks.7.attention.dense3.weight",
      "backbone.blocks.7.output.mapping.weight",
      "backbone.blocks.7.output.projection.weight",
      "backbone.blocks.8.attention.projection.weight",
      "backbone.blocks.8.attention.dense1.weight",
      "backbone.blocks.8.attention.dense2.weight",
      "backbone.blocks.8.attention.dense3.weight",
      "backbone.blocks.8.output.mapping.weight",
      "backbone.blocks.8.output.projection.weight",
      "backbone.blocks.9.attention.projection.weight",
      "backbone.blocks.9.attention.dense1.weight",
      "backbone.blocks.9.attention.dense2.weight",
      "backbone.blocks.9.attention.dense3.weight",
      "backbone.blocks.9.output.mapping.weight",
      "backbone.blocks.9.output.projection.weight",
      "backbone.blocks.10.attention.projection.weight",
      "backbone.blocks.10.attention.dense1.weight",
      "backbone.blocks.10.attention.dense2.weight",
      "backbone.blocks.10.attention.dense3.weight",
      "backbone.blocks.10.output.mapping.weight",
      "backbone.blocks.10.output.projection.weight",
      "backbone.blocks.11.attention.projection.weight",
      "backbone.blocks.11.attention.dense1.weight",
      "backbone.blocks.11.attention.dense2.weight",
      "backbone.blocks.11.attention.dense3.weight",
      "backbone.blocks.11.output.mapping.weight",
      "backbone.blocks.11.output.projection.weight"
    ]
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "backbone.layernorm.gamma",
      "backbone.layernorm.beta",
      "backbone.blocks.0.layernorm1.gamma",
      "backbone.blocks.0.layernorm1.beta",
      "backbone.blocks.0.layernorm2.gamma",
      "backbone.blocks.0.layernorm2.beta",
      "backbone.blocks.0.attention.projection.bias",
      "backbone.blocks.0.attention.dense1.bias",
      "backbone.blocks.0.attention.dense2.bias",
      "backbone.blocks.0.attention.dense3.bias",
      "backbone.blocks.0.output.mapping.bias",
      "backbone.blocks.0.output.projection.bias",
      "backbone.blocks.1.layernorm1.gamma",
      "backbone.blocks.1.layernorm1.beta",
      "backbone.blocks.1.layernorm2.gamma",
      "backbone.blocks.1.layernorm2.beta",
      "backbone.blocks.1.attention.projection.bias",
      "backbone.blocks.1.attention.dense1.bias",
      "backbone.blocks.1.attention.dense2.bias",
      "backbone.blocks.1.attention.dense3.bias",
      "backbone.blocks.1.output.mapping.bias",
      "backbone.blocks.1.output.projection.bias",
      "backbone.blocks.2.layernorm1.gamma",
      "backbone.blocks.2.layernorm1.beta",
      "backbone.blocks.2.layernorm2.gamma",
      "backbone.blocks.2.layernorm2.beta",
      "backbone.blocks.2.attention.projection.bias",
      "backbone.blocks.2.attention.dense1.bias",
      "backbone.blocks.2.attention.dense2.bias",
      "backbone.blocks.2.attention.dense3.bias",
      "backbone.blocks.2.output.mapping.bias",
      "backbone.blocks.2.output.projection.bias",
      "backbone.blocks.3.layernorm1.gamma",
      "backbone.blocks.3.layernorm1.beta",
      "backbone.blocks.3.layernorm2.gamma",
      "backbone.blocks.3.layernorm2.beta",
      "backbone.blocks.3.attention.projection.bias",
      "backbone.blocks.3.attention.dense1.bias",
      "backbone.blocks.3.attention.dense2.bias",
      "backbone.blocks.3.attention.dense3.bias",
      "backbone.blocks.3.output.mapping.bias",
      "backbone.blocks.3.output.projection.bias",
      "backbone.blocks.4.layernorm1.gamma",
      "backbone.blocks.4.layernorm1.beta",
      "backbone.blocks.4.layernorm2.gamma",
      "backbone.blocks.4.layernorm2.beta",
      "backbone.blocks.4.attention.projection.bias",
      "backbone.blocks.4.attention.dense1.bias",
      "backbone.blocks.4.attention.dense2.bias",
      "backbone.blocks.4.attention.dense3.bias",
      "backbone.blocks.4.output.mapping.bias",
      "backbone.blocks.4.output.projection.bias",
      "backbone.blocks.5.layernorm1.gamma",
      "backbone.blocks.5.layernorm1.beta",
      "backbone.blocks.5.layernorm2.gamma",
      "backbone.blocks.5.layernorm2.beta",
      "backbone.blocks.5.attention.projection.bias",
      "backbone.blocks.5.attention.dense1.bias",
      "backbone.blocks.5.attention.dense2.bias",
      "backbone.blocks.5.attention.dense3.bias",
      "backbone.blocks.5.output.mapping.bias",
      "backbone.blocks.5.output.projection.bias",
      "backbone.blocks.6.layernorm1.gamma",
      "backbone.blocks.6.layernorm1.beta",
      "backbone.blocks.6.layernorm2.gamma",
      "backbone.blocks.6.layernorm2.beta",
      "backbone.blocks.6.attention.projection.bias",
      "backbone.blocks.6.attention.dense1.bias",
      "backbone.blocks.6.attention.dense2.bias",
      "backbone.blocks.6.attention.dense3.bias",
      "backbone.blocks.6.output.mapping.bias",
      "backbone.blocks.6.output.projection.bias",
      "backbone.blocks.7.layernorm1.gamma",
      "backbone.blocks.7.layernorm1.beta",
      "backbone.blocks.7.layernorm2.gamma",
      "backbone.blocks.7.layernorm2.beta",
      "backbone.blocks.7.attention.projection.bias",
      "backbone.blocks.7.attention.dense1.bias",
      "backbone.blocks.7.attention.dense2.bias",
      "backbone.blocks.7.attention.dense3.bias",
      "backbone.blocks.7.output.mapping.bias",
      "backbone.blocks.7.output.projection.bias",
      "backbone.blocks.8.layernorm1.gamma",
      "backbone.blocks.8.layernorm1.beta",
      "backbone.blocks.8.layernorm2.gamma",
      "backbone.blocks.8.layernorm2.beta",
      "backbone.blocks.8.attention.projection.bias",
      "backbone.blocks.8.attention.dense1.bias",
      "backbone.blocks.8.attention.dense2.bias",
      "backbone.blocks.8.attention.dense3.bias",
      "backbone.blocks.8.output.mapping.bias",
      "backbone.blocks.8.output.projection.bias",
      "backbone.blocks.9.layernorm1.gamma",
      "backbone.blocks.9.layernorm1.beta",
      "backbone.blocks.9.layernorm2.gamma",
      "backbone.blocks.9.layernorm2.beta",
      "backbone.blocks.9.attention.projection.bias",
      "backbone.blocks.9.attention.dense1.bias",
      "backbone.blocks.9.attention.dense2.bias",
      "backbone.blocks.9.attention.dense3.bias",
      "backbone.blocks.9.output.mapping.bias",
      "backbone.blocks.9.output.projection.bias",
      "backbone.blocks.10.layernorm1.gamma",
      "backbone.blocks.10.layernorm1.beta",
      "backbone.blocks.10.layernorm2.gamma",
      "backbone.blocks.10.layernorm2.beta",
      "backbone.blocks.10.attention.projection.bias",
      "backbone.blocks.10.attention.dense1.bias",
      "backbone.blocks.10.attention.dense2.bias",
      "backbone.blocks.10.attention.dense3.bias",
      "backbone.blocks.10.output.mapping.bias",
      "backbone.blocks.10.output.projection.bias",
      "backbone.blocks.11.layernorm1.gamma",
      "backbone.blocks.11.layernorm1.beta",
      "backbone.blocks.11.layernorm2.gamma",
      "backbone.blocks.11.layernorm2.beta",
      "backbone.blocks.11.attention.projection.bias",
      "backbone.blocks.11.attention.dense1.bias",
      "backbone.blocks.11.attention.dense2.bias",
      "backbone.blocks.11.attention.dense3.bias",
      "backbone.blocks.11.output.mapping.bias",
      "backbone.blocks.11.output.projection.bias"
    ]
  }
}
2024-01-29 05:53:03,876 - mindformers[mindformers/trainer/base_trainer.py:527] - INFO - Network Parameters: 124 M.
2024-01-29 05:53:03,877 - mindformers[mindformers/trainer/base_trainer.py:658] - INFO - .........Build Optimizer For Train..........
2024-01-29 05:53:03,877 - mindformers[mindformers/trainer/base_trainer.py:423] - INFO - .........Build Optimizer From Config..........
2024-01-29 05:53:03,877 - mindformers[mindformers/trainer/base_trainer.py:456] - INFO - .........Build LR Schedule From Config..........
2024-01-29 05:53:03,880 - mindformers[mindformers/trainer/optimizer_grouped_parameters.py:74] - WARNING - dynamic_lr_schedule will be reset and invalid when layer_scale is False.
2024-01-29 05:53:03,882 - mindformers[mindformers/trainer/optimizer_grouped_parameters.py:113] - INFO - Param groups = {
  "decay": {
    "weight_decay": 0.1,
    "params": [
      "backbone.embedding.word_embedding.embedding_table",
      "backbone.embedding.position_embedding.embedding_table",
      "backbone.blocks.0.attention.projection.weight",
      "backbone.blocks.0.attention.dense1.weight",
      "backbone.blocks.0.attention.dense2.weight",
      "backbone.blocks.0.attention.dense3.weight",
      "backbone.blocks.0.output.mapping.weight",
      "backbone.blocks.0.output.projection.weight",
      "backbone.blocks.1.attention.projection.weight",
      "backbone.blocks.1.attention.dense1.weight",
      "backbone.blocks.1.attention.dense2.weight",
      "backbone.blocks.1.attention.dense3.weight",
      "backbone.blocks.1.output.mapping.weight",
      "backbone.blocks.1.output.projection.weight",
      "backbone.blocks.2.attention.projection.weight",
      "backbone.blocks.2.attention.dense1.weight",
      "backbone.blocks.2.attention.dense2.weight",
      "backbone.blocks.2.attention.dense3.weight",
      "backbone.blocks.2.output.mapping.weight",
      "backbone.blocks.2.output.projection.weight",
      "backbone.blocks.3.attention.projection.weight",
      "backbone.blocks.3.attention.dense1.weight",
      "backbone.blocks.3.attention.dense2.weight",
      "backbone.blocks.3.attention.dense3.weight",
      "backbone.blocks.3.output.mapping.weight",
      "backbone.blocks.3.output.projection.weight",
      "backbone.blocks.4.attention.projection.weight",
      "backbone.blocks.4.attention.dense1.weight",
      "backbone.blocks.4.attention.dense2.weight",
      "backbone.blocks.4.attention.dense3.weight",
      "backbone.blocks.4.output.mapping.weight",
      "backbone.blocks.4.output.projection.weight",
      "backbone.blocks.5.attention.projection.weight",
      "backbone.blocks.5.attention.dense1.weight",
      "backbone.blocks.5.attention.dense2.weight",
      "backbone.blocks.5.attention.dense3.weight",
      "backbone.blocks.5.output.mapping.weight",
      "backbone.blocks.5.output.projection.weight",
      "backbone.blocks.6.attention.projection.weight",
      "backbone.blocks.6.attention.dense1.weight",
      "backbone.blocks.6.attention.dense2.weight",
      "backbone.blocks.6.attention.dense3.weight",
      "backbone.blocks.6.output.mapping.weight",
      "backbone.blocks.6.output.projection.weight",
      "backbone.blocks.7.attention.projection.weight",
      "backbone.blocks.7.attention.dense1.weight",
      "backbone.blocks.7.attention.dense2.weight",
      "backbone.blocks.7.attention.dense3.weight",
      "backbone.blocks.7.output.mapping.weight",
      "backbone.blocks.7.output.projection.weight",
      "backbone.blocks.8.attention.projection.weight",
      "backbone.blocks.8.attention.dense1.weight",
      "backbone.blocks.8.attention.dense2.weight",
      "backbone.blocks.8.attention.dense3.weight",
      "backbone.blocks.8.output.mapping.weight",
      "backbone.blocks.8.output.projection.weight",
      "backbone.blocks.9.attention.projection.weight",
      "backbone.blocks.9.attention.dense1.weight",
      "backbone.blocks.9.attention.dense2.weight",
      "backbone.blocks.9.attention.dense3.weight",
      "backbone.blocks.9.output.mapping.weight",
      "backbone.blocks.9.output.projection.weight",
      "backbone.blocks.10.attention.projection.weight",
      "backbone.blocks.10.attention.dense1.weight",
      "backbone.blocks.10.attention.dense2.weight",
      "backbone.blocks.10.attention.dense3.weight",
      "backbone.blocks.10.output.mapping.weight",
      "backbone.blocks.10.output.projection.weight",
      "backbone.blocks.11.attention.projection.weight",
      "backbone.blocks.11.attention.dense1.weight",
      "backbone.blocks.11.attention.dense2.weight",
      "backbone.blocks.11.attention.dense3.weight",
      "backbone.blocks.11.output.mapping.weight",
      "backbone.blocks.11.output.projection.weight"
    ]
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "backbone.layernorm.gamma",
      "backbone.layernorm.beta",
      "backbone.blocks.0.layernorm1.gamma",
      "backbone.blocks.0.layernorm1.beta",
      "backbone.blocks.0.layernorm2.gamma",
      "backbone.blocks.0.layernorm2.beta",
      "backbone.blocks.0.attention.projection.bias",
      "backbone.blocks.0.attention.dense1.bias",
      "backbone.blocks.0.attention.dense2.bias",
      "backbone.blocks.0.attention.dense3.bias",
      "backbone.blocks.0.output.mapping.bias",
      "backbone.blocks.0.output.projection.bias",
      "backbone.blocks.1.layernorm1.gamma",
      "backbone.blocks.1.layernorm1.beta",
      "backbone.blocks.1.layernorm2.gamma",
      "backbone.blocks.1.layernorm2.beta",
      "backbone.blocks.1.attention.projection.bias",
      "backbone.blocks.1.attention.dense1.bias",
      "backbone.blocks.1.attention.dense2.bias",
      "backbone.blocks.1.attention.dense3.bias",
      "backbone.blocks.1.output.mapping.bias",
      "backbone.blocks.1.output.projection.bias",
      "backbone.blocks.2.layernorm1.gamma",
      "backbone.blocks.2.layernorm1.beta",
      "backbone.blocks.2.layernorm2.gamma",
      "backbone.blocks.2.layernorm2.beta",
      "backbone.blocks.2.attention.projection.bias",
      "backbone.blocks.2.attention.dense1.bias",
      "backbone.blocks.2.attention.dense2.bias",
      "backbone.blocks.2.attention.dense3.bias",
      "backbone.blocks.2.output.mapping.bias",
      "backbone.blocks.2.output.projection.bias",
      "backbone.blocks.3.layernorm1.gamma",
      "backbone.blocks.3.layernorm1.beta",
      "backbone.blocks.3.layernorm2.gamma",
      "backbone.blocks.3.layernorm2.beta",
      "backbone.blocks.3.attention.projection.bias",
      "backbone.blocks.3.attention.dense1.bias",
      "backbone.blocks.3.attention.dense2.bias",
      "backbone.blocks.3.attention.dense3.bias",
      "backbone.blocks.3.output.mapping.bias",
      "backbone.blocks.3.output.projection.bias",
      "backbone.blocks.4.layernorm1.gamma",
      "backbone.blocks.4.layernorm1.beta",
      "backbone.blocks.4.layernorm2.gamma",
      "backbone.blocks.4.layernorm2.beta",
      "backbone.blocks.4.attention.projection.bias",
      "backbone.blocks.4.attention.dense1.bias",
      "backbone.blocks.4.attention.dense2.bias",
      "backbone.blocks.4.attention.dense3.bias",
      "backbone.blocks.4.output.mapping.bias",
      "backbone.blocks.4.output.projection.bias",
      "backbone.blocks.5.layernorm1.gamma",
      "backbone.blocks.5.layernorm1.beta",
      "backbone.blocks.5.layernorm2.gamma",
      "backbone.blocks.5.layernorm2.beta",
      "backbone.blocks.5.attention.projection.bias",
      "backbone.blocks.5.attention.dense1.bias",
      "backbone.blocks.5.attention.dense2.bias",
      "backbone.blocks.5.attention.dense3.bias",
      "backbone.blocks.5.output.mapping.bias",
      "backbone.blocks.5.output.projection.bias",
      "backbone.blocks.6.layernorm1.gamma",
      "backbone.blocks.6.layernorm1.beta",
      "backbone.blocks.6.layernorm2.gamma",
      "backbone.blocks.6.layernorm2.beta",
      "backbone.blocks.6.attention.projection.bias",
      "backbone.blocks.6.attention.dense1.bias",
      "backbone.blocks.6.attention.dense2.bias",
      "backbone.blocks.6.attention.dense3.bias",
      "backbone.blocks.6.output.mapping.bias",
      "backbone.blocks.6.output.projection.bias",
      "backbone.blocks.7.layernorm1.gamma",
      "backbone.blocks.7.layernorm1.beta",
      "backbone.blocks.7.layernorm2.gamma",
      "backbone.blocks.7.layernorm2.beta",
      "backbone.blocks.7.attention.projection.bias",
      "backbone.blocks.7.attention.dense1.bias",
      "backbone.blocks.7.attention.dense2.bias",
      "backbone.blocks.7.attention.dense3.bias",
      "backbone.blocks.7.output.mapping.bias",
      "backbone.blocks.7.output.projection.bias",
      "backbone.blocks.8.layernorm1.gamma",
      "backbone.blocks.8.layernorm1.beta",
      "backbone.blocks.8.layernorm2.gamma",
      "backbone.blocks.8.layernorm2.beta",
      "backbone.blocks.8.attention.projection.bias",
      "backbone.blocks.8.attention.dense1.bias",
      "backbone.blocks.8.attention.dense2.bias",
      "backbone.blocks.8.attention.dense3.bias",
      "backbone.blocks.8.output.mapping.bias",
      "backbone.blocks.8.output.projection.bias",
      "backbone.blocks.9.layernorm1.gamma",
      "backbone.blocks.9.layernorm1.beta",
      "backbone.blocks.9.layernorm2.gamma",
      "backbone.blocks.9.layernorm2.beta",
      "backbone.blocks.9.attention.projection.bias",
      "backbone.blocks.9.attention.dense1.bias",
      "backbone.blocks.9.attention.dense2.bias",
      "backbone.blocks.9.attention.dense3.bias",
      "backbone.blocks.9.output.mapping.bias",
      "backbone.blocks.9.output.projection.bias",
      "backbone.blocks.10.layernorm1.gamma",
      "backbone.blocks.10.layernorm1.beta",
      "backbone.blocks.10.layernorm2.gamma",
      "backbone.blocks.10.layernorm2.beta",
      "backbone.blocks.10.attention.projection.bias",
      "backbone.blocks.10.attention.dense1.bias",
      "backbone.blocks.10.attention.dense2.bias",
      "backbone.blocks.10.attention.dense3.bias",
      "backbone.blocks.10.output.mapping.bias",
      "backbone.blocks.10.output.projection.bias",
      "backbone.blocks.11.layernorm1.gamma",
      "backbone.blocks.11.layernorm1.beta",
      "backbone.blocks.11.layernorm2.gamma",
      "backbone.blocks.11.layernorm2.beta",
      "backbone.blocks.11.attention.projection.bias",
      "backbone.blocks.11.attention.dense1.bias",
      "backbone.blocks.11.attention.dense2.bias",
      "backbone.blocks.11.attention.dense3.bias",
      "backbone.blocks.11.output.mapping.bias",
      "backbone.blocks.11.output.projection.bias"
    ]
  }
}
2024-01-29 05:53:04,101 - mindformers[mindformers/trainer/base_trainer.py:664] - INFO - .........Build Running Wrapper From Config For Train..........
2024-01-29 05:53:04,101 - mindformers[mindformers/trainer/base_trainer.py:493] - INFO - .........Build Model Wrapper for Train From Config..........
2024-01-29 05:53:04,112 - mindformers[mindformers/trainer/base_trainer.py:671] - INFO - .........Build Callbacks For Train..........
2024-01-29 05:53:04,112 - mindformers[mindformers/trainer/base_trainer.py:502] - INFO - .........Build Callbacks for Train From Config..........
2024-01-29 05:53:04,113 - mindformers[mindformers/core/callback/callback.py:530] - INFO - Integrated_save is changed to False when using auto_parallel.
2024-01-29 05:53:04,114 - mindformers[mindformers/trainer/base_trainer.py:695] - INFO - .........Starting Init Train Model..........
2024-01-29 05:53:04,114 - mindformers[mindformers/trainer/base_trainer.py:730] - INFO - .........Starting Training Model..........
{'auto_trans_ckpt': False,
 'auto_tune': False,
 'autotune_per_step': 10,
 'callbacks': [OrderedDict([('type', 'MFLossMonitor')]),
               OrderedDict([('type', 'CheckpointMointor'),
                            ('prefix', 'gpt'),
                            ('save_checkpoint_steps', 1),
                            ('integrated_save', True),
                            ('save_network_params', True),
                            ('save_trainable_params', False),
                            ('async_save', False)]),
               OrderedDict([('type', 'ObsMonitor')])],
 'context': {'device_id': 0, 'device_target': 'Ascend', 'mode': 0},
 'data_seed': None,
 'data_size': 1099,
 'device_num': 2,
 'do_eval': False,
 'eval_callbacks': [OrderedDict([('type', 'ObsMonitor')])],
 'eval_dataset': {'auto_tune': False,
                  'autotune_per_step': 10,
                  'batch_size': 1,
                  'data_loader': {'dataset_dir': '',
                                  'shuffle': False,
                                  'type': 'MindDataset'},
                  'do_eval': True,
                  'drop_remainder': False,
                  'filepath_prefix': './autotune',
                  'input_columns': ['input_ids', 'attention_mask'],
                  'num_parallel_workers': 8,
                  'numa_enable': False,
                  'prefetch_size': 1,
                  'profile': False,
                  'python_multiprocessing': False,
                  'repeat': 1,
                  'seed': 0},
 'eval_dataset_task': {'dataset_config': {'auto_tune': False,
                                          'autotune_per_step': 10,
                                          'batch_size': 1,
                                          'data_loader': {'dataset_dir': '',
                                                          'shuffle': False,
                                                          'type': 'MindDataset'},
                                          'do_eval': True,
                                          'drop_remainder': False,
                                          'filepath_prefix': './autotune',
                                          'input_columns': ['input_ids',
                                                            'attention_mask'],
                                          'num_parallel_workers': 8,
                                          'numa_enable': False,
                                          'prefetch_size': 1,
                                          'profile': False,
                                          'python_multiprocessing': False,
                                          'repeat': 1,
                                          'seed': 0},
                       'type': 'CausalLanguageModelDataset'},
 'eval_epoch_interval': None,
 'eval_step_interval': None,
 'filepath_prefix': './autotune',
 'init_start_profile': False,
 'layer_decay': 0.65,
 'layer_scale': False,
 'load_checkpoint': '',
 'lr_scale': False,
 'lr_scale_factor': 256,
 'lr_schedule': {'learning_rate': 0.0001,
                 'lr_end': 1e-05,
                 'total_steps': 1098,
                 'type': 'polynomial',
                 'warmup_steps': 0},
 'metric': {'type': 'PerplexityMetric'},
 'micro_batch_interleave_num': 1,
 'model': {'arch': {'type': 'GPT2LMHeadModel'},
           'model_config': {'attention_dropout_rate': 0.1,
                            'checkpoint_name_or_path': None,
                            'compute_dtype': 'float16',
                            'do_sample': True,
                            'eos_token_id': 50256,
                            'expand_ratio': 4,
                            'hidden_act': 'gelu',
                            'hidden_dropout_rate': 0.1,
                            'hidden_size': 768,
                            'layernorm_compute_type': 'float32',
                            'max_decode_length': 1024,
                            'num_heads': 12,
                            'num_layers': 12,
                            'param_init_type': 'float32',
                            'repetition_penalty': 1,
                            'seq_length': 1024,
                            'softmax_compute_type': 'float32',
                            'top_k': 5,
                            'top_p': 1,
                            'type': 'GPT2Config',
                            'use_flash_attention': False,
                            'use_past': False,
                            'vocab_size': 50257}},
 'moe_config': <mindformers.modules.transformer.moe.MoEConfig object at 0x7f7fd8dfdf90>,
 'only_save_strategy': False,
 'optimizer': {'beta1': 0.9,
               'beta2': 0.95,
               'eps': 1e-08,
               'type': 'FusedAdamWeightDecay',
               'weight_decay': 0.1},
 'output_dir': './output',
 'parallel': {'enable_parallel_optimizer': False,
              'gradients_mean': True,
              'parallel_mode': 0,
              'search_mode': 'sharding_propagation',
              'strategy_ckpt_save_file': './output/strategy/ckpt_strategy_rank_0.ckpt'},
 'parallel_config': <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object at 0x7f7fa40501d0>,
 'processor': {'return_tensors': 'ms',
               'tokenizer': {'bos_token': '<|endoftext|>',
                             'eos_token': '<|endoftext|>',
                             'pad_token': '<|endoftext|>',
                             'type': 'GPT2Tokenizer',
                             'unk_token': '<|endoftext|>'},
               'type': 'GPT2Processor'},
 'profile': False,
 'profile_communication': False,
 'profile_memory': True,
 'profile_start_step': 1,
 'profile_stop_step': 10,
 'rank_id': 0,
 'recompute_config': <mindformers.modules.transformer.transformer.TransformerRecomputeConfig object at 0x7f7fd8dfd410>,
 'remote_save_url': 'Please input obs url on AICC platform.',
 'resume_training': False,
 'run_mode': 'train',
 'runner_config': {'batch_size': 1,
                   'epochs': 549,
                   'gradient_accumulation_steps': 1,
                   'initial_epoch': 0,
                   'initial_step': 0,
                   'origin_epochs': 1,
                   'per_epoch_size': None,
                   'sink_mode': True,
                   'sink_size': 2},
 'runner_wrapper': {'scale_sense': DynamicLossScaleUpdateCell<>,
                    'type': 'MFTrainOneStepCell',
                    'use_clip_grad': True},
 'seed': 0,
 'src_strategy_path_or_dir': '',
 'train_dataset': {'auto_tune': False,
                   'autotune_per_step': 10,
                   'batch_size': 1,
                   'data_loader': {'dataset_dir': 'data/wikitext-2/wikitext-2.mindrecord',
                                   'shuffle': True,
                                   'type': 'MindDataset'},
                   'do_eval': False,
                   'drop_remainder': True,
                   'filepath_prefix': './autotune',
                   'input_columns': ['input_ids', 'attention_mask'],
                   'num_parallel_workers': 8,
                   'numa_enable': False,
                   'prefetch_size': 1,
                   'profile': False,
                   'python_multiprocessing': False,
                   'repeat': 1,
                   'seed': 0},
 'train_dataset_task': {'dataset_config': {'auto_tune': False,
                                           'autotune_per_step': 10,
                                           'batch_size': 1,
                                           'data_loader': {'dataset_dir': 'data/wikitext-2/wikitext-2.mindrecord',
                                                           'shuffle': True,
                                                           'type': 'MindDataset'},
                                           'do_eval': False,
                                           'drop_remainder': True,
                                           'filepath_prefix': './autotune',
                                           'input_columns': ['input_ids',
                                                             'attention_mask'],
                                           'num_parallel_workers': 8,
                                           'numa_enable': False,
                                           'prefetch_size': 1,
                                           'profile': False,
                                           'python_multiprocessing': False,
                                           'repeat': 1,
                                           'seed': 0},
                        'type': 'CausalLanguageModelDataset'},
 'trainer': {'model_name': 'GPT2LMHeadModel',
             'type': 'CausalLanguageModelingTrainer'},
 'use_parallel': False}

vim /yjy/test/configs/gpt2/run_gpt2.yaml

预处理数据：
python mindformers/mindformers/tools/dataset_preprocess/gpt2/wikitext2_data_process.py \
                              --input_file test/data/wikitext-2/wiki.train.tokens \
                              --output_file test/data/wikitext-2/wikitext-2.train.mindrecord \
                              --max_length 1025
训练：
CUDA_VISIBLE_DEVICES=4,5,6,7 mpirun -n 4 --allow-run-as-root \
                         python mindformers/run_mindformer.py --config /yjy/test/configs/gpt2/run_gpt2.yaml \
                         --run_mode train \
                         --train_dataset_dir /yjy/test/data/wikitext-2/wikitext-2.train.mindrecord

推理代码：
权重转换
python mindformers/mindformers/models/gpt2/convert_weight.py --layers 12 --torch_path gpt2_small.bin --mindspore_path ./gpt2_small.ckpt

数据预处理：
python create_pretraining_data.py \
  --input_file=test/data/en.txt \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5

CUDA_VISIBLE_DEVICES=0,1 mpirun -n 2 --allow-run-as-root \
                         python mindformers/run_mindformer.py --config /yjy/test/configs/gpt2/run_gpt2.yaml \
                         --run_mode train \
                         --train_dataset_dir /yjy/test/data/wikitext-2/wikitext-2.train.mindrecord

vim mindspore/mindspore/ccsrc/frontend/parallel/step_auto_parallel.cc

cd mindspore
bash build.sh -e gpu -S on
pip install output/mindspore-*.whl -i https://pypi.tuna.tsinghua.edu.cn/simple
dynamic_programming
sharding_propagation
recursive_programming


operator_info中各属性的含义：
  std::string name_;
  Shapes inputs_shape_;
  Shapes outputs_shape_;
  mindspore::HashMap<std::string, ValuePtr> attrs_;
  std::vector<ValuePtr> input_value_;
  TypePtr outputs_dtype_;

  int32_t stage_id_ = 0;
  StrategyPtr strategy_;
  StrategyPtr out_strategy_;
  std::vector<TensorInfo> inputs_tensor_info_;
  std::vector<TensorInfo> outputs_tensor_info_;
  Shape dev_matrix_shape_;  // if repeated calculation, it contains the repeated_calc_num_
  Shape out_dev_matrix_shape_;
  int64_t repeated_calc_num_ = 1;
  int64_t as_loss_divisor_ = 1;
  TensorMaps inputs_tensor_map_;
  TensorMaps outputs_tensor_map_;
  ForwardOp forward_op_;
  Ops sub_ops_;
  ForwardOp replace_op_;
  OutPutInfoVector replace_op_info_;
  ReplaceGraphPtr replace_graph_;
  MirrorOps mirror_ops_;
  VirtualDivOp virtual_div_op_;
  RankList stage_device_list_;  // the device list in this stage
  int64_t stage_device_size_ = 0;
  bool infer_attrs_completed_ = false;
  bool is_layout_config_ = false;
  Shapes strategy_from_layout_;

  bool is_auto_parallel_ = false;      // false: semi_auto_parallel; true: auto_parallel
  bool is_assigned_parallel_ = false;  // false: origin parallel; true: dynamic_shape parallel
  // 'corrected_input_indices_' used to store the indices of input that have ALREADY been corrected.
  std::vector<size_t> corrected_input_indices_;
  // Given a parallelization strategy, there is a cost.
  std::vector<std::shared_ptr<StrategyWithCost>> strategy_cost_;
  std::string involved_param_name_;
  // For each input in 'inputs_', there is a bool variable indicating whether that the corresponding input is parameter
  std::vector<bool> is_parameter_;
  // For each input in 'inputs_', a bool variable is true if the corresponding one is a parameter or a output of
  // pre-operator that has parameters as input.
  std::vector<bool> is_parameter_involve_;
  // If any input is parameter-involved, the output is parameter-involved. This variable is used in calculating
  // peak memory cost in the training phase.
  // -1: unset; 0: not parameter_involved; 1: parameter_involved
  int64_t is_output_parameter_involve_ = -1;
  // Whether this output is critical, which means that this output is included in calculating peak memory cost
  // in the inference phase.
  // -1 : unset; 0: not critical; 1: critical
  int64_t is_output_critical_ = -1;
  double outputs_total_size_ = 0.0;
  bool is_calculated_outputs_size_ = false;
  // for each input and output, the followings record the number of bytes of each element
  std::vector<size_t> inputs_type_lengths_;
  std::vector<size_t> outputs_type_lengths_;
  std::vector<std::shared_ptr<Edge>> prev_edges_;
  std::vector<std::shared_ptr<Edge>> succ_edges_;
  StrategyPtr selected_strategy_;
  int64_t selected_strategy_depth_ = -1;
  // Used in DP algorithm
  bool is_alive_;
  CostPtr selected_cost_;
  std::vector<bool> split_flag_list_;
  std::string refkey_parameter_name_;
  CNodePtr cnode_;
  std::vector<CNodePtr> cnodes_;
  int64_t used_devices_ = -1;
  // the repeated_calc_num_ will be inserted to the last dimension of dev matrix in default
  bool repeated_num_in_dev_matrix_right_ = true;
  // Whether the list of available strategies is exact or approximate
  bool is_strategy_cost_exact_ = true;

 private:
  OperatorCostPtr operator_cost_;
  std::vector<TypePtr> outputs_type_;
  int64_t swc_index_ = -1;
  Status GetLayoutConfig();
  Status CheckLayoutConfigBase();


  cost_graph的属性：
  // Needed by rec_parser
  std::vector<std::vector<std::string>> inputs_tensor_name_list_;
  // Needed by rec_parser 2
  std::vector<std::vector<std::string>> param_users_uniqueid_list_;
  std::map<std::string, std::string> tuple_getitem_list_;
  std::vector<OperatorInfoPtr> ops_;
  std::map<std::pair<OperatorInfoPtr, OperatorInfoPtr>, std::vector<EdgePtr>> edges_;
  std::vector<std::shared_ptr<CostGraph>> connected_compoents_;
  std::map<OperatorInfoPtr, std::vector<EdgePtr>> out_edges_;
  std::map<OperatorInfoPtr, std::vector<EdgePtr>> in_edges_;
  和gs的cost——graph有区别不能直接替换，遍历node

gs：-------------
class CostGraph {
  private:
    std::vector<CostNode> cost_nodes;
    std::map<std::string, CostNode&> cost_node_map;


  gs中的costnode：
    std::string name;
    std::string device;
    // operator_costmodel.h: GetMemoryCost,GetComputationCost
    int64_t compute_cost;
    int64_t memory_cost;
    int64_t output_memory;
    std::vector<std::string> inputs;
    std::vector<std::string> outputs;
    std::vector<int64_t> input_comm_costs;
    std::vector<int64_t> output_comm_costs;
    int64_t start_time;
    int64_t end_time;


排布模型由device_matrix，tensor_shape和tensor map组成，分别表示设备矩阵形状、张量形状、设备和张量维度间的映射关系。

调用ParallelStrategySearch或者SearchParallelStrategy的接口是哪个